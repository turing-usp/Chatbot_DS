{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5b7313",
   "metadata": {},
   "source": [
    "Implementa o agente inspetor de codigo que verifica o codigo gerado no node anterior e:\n",
    "\n",
    "i. avanca para a execucao do codigo se o avaliar positivamente\n",
    "\n",
    "ii. retorna (feedback loop) para o node anterior com um feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06513df8",
   "metadata": {},
   "source": [
    "coloque sua chave do gemini api para rodar!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968a288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"user_input\": \"NAO ESTAMOS USANDO PROMPT AGR\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate_code] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"messages\": [],\n",
      "  \"user_input\": \"NAO ESTAMOS USANDO PROMPT AGR\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate_code > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate_code > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Crie um codigo em python simples com um erro dificil de detectar\\nSua resposta deve conter apenas o codigo\\nResponda com {code: string}.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate_code > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [4.59s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.5-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"name\": \"Code\",\n",
      "                \"arguments\": \"{\\\"code\\\": \\\"def add_to_list(item, my_list=[]):\\\\n    my_list.append(item)\\\\n    return my_list\\\\n\\\\nprint(add_to_list(1))\\\\nprint(add_to_list(2))\\\\nprint(add_to_list(3, []))\\\\nprint(add_to_list(4))\\\"}\"\n",
      "              }\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.5-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--c890cadd-7116-4b6c-9ded-4518d67a3235-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"Code\",\n",
      "                \"args\": {\n",
      "                  \"code\": \"def add_to_list(item, my_list=[]):\\n    my_list.append(item)\\n    return my_list\\n\\nprint(add_to_list(1))\\nprint(add_to_list(2))\\nprint(add_to_list(3, []))\\nprint(add_to_list(4))\"\n",
      "                },\n",
      "                \"id\": \"9b27158b-9e0c-43b7-bf66-3699bf62c2b8\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 76,\n",
      "              \"output_tokens\": 576,\n",
      "              \"total_tokens\": 652,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"reasoning\": 485\n",
      "              }\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate_code > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate_code > chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate_code > chain:RunnableSequence] [4.59s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate_code] [4.59s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Você é um revisor de código estrito. Avalie se o código está correto, legível e executável.\\nSe encontrar problemas, reprove e dê feedback objetivo e curto; se estiver ok, aprove.\\nHuman: Inspecione o código a seguir.\\nResponda com {approved: boolean, feedback: string}.\\n\\n```python\\ncode='def add_to_list(item, my_list=[]):\\\\n    my_list.append(item)\\\\n    return my_list\\\\n\\\\nprint(add_to_list(1))\\\\nprint(add_to_list(2))\\\\nprint(add_to_list(3, []))\\\\nprint(add_to_list(4))'\\n```\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [2.60s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.5-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"name\": \"InspectorVerdict\",\n",
      "                \"arguments\": \"{\\\"approved\\\": false, \\\"feedback\\\": \\\"Reprovado: O uso de um objeto mut\\\\u00e1vel (lista) como argumento padr\\\\u00e3o pode levar a comportamentos inesperados, pois o objeto \\\\u00e9 compartilhado entre as chamadas da fun\\\\u00e7\\\\u00e3o. Use 'None' como padr\\\\u00e3o e inicialize a lista dentro da fun\\\\u00e7\\\\u00e3o se 'None' for passado.\\\"}\"\n",
      "              }\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.5-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--2479a2e1-34bc-4c05-a09a-912a7296f551-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"InspectorVerdict\",\n",
      "                \"args\": {\n",
      "                  \"approved\": false,\n",
      "                  \"feedback\": \"Reprovado: O uso de um objeto mutável (lista) como argumento padrão pode levar a comportamentos inesperados, pois o objeto é compartilhado entre as chamadas da função. Use 'None' como padrão e inicialize a lista dentro da função se 'None' for passado.\"\n",
      "                },\n",
      "                \"id\": \"d0388a30-1958-484f-8849-956719b1cc1d\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 223,\n",
      "              \"output_tokens\": 401,\n",
      "              \"total_tokens\": 624,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"reasoning\": 326\n",
      "              }\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence] [2.60s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "Reprovado: O uso de um objeto mutável (lista) como argumento padrão pode levar a comportamentos inesperados, pois o objeto é compartilhado entre as chamadas da função. Use 'None' como padrão e inicialize a lista dentro da função se 'None' for passado.\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:inspect_routing_function] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:inspect_routing_function] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"revise\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector] [2.61s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:revise_code] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:revise_code > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:revise_code > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Você é um corretor de código. Baseado no feedback a seguir corrija o codigo.\\n\\nHuman: Corrija o código a seguir baseado no feedback.\\nResponda com {code: string}.\\n\\n```feedback\\nReprovado: O uso de um objeto mutável (lista) como argumento padrão pode levar a comportamentos inesperados, pois o objeto é compartilhado entre as chamadas da função. Use 'None' como padrão e inicialize a lista dentro da função se 'None' for passado.\\n``````python\\ncode='def add_to_list(item, my_list=[]):\\\\n    my_list.append(item)\\\\n    return my_list\\\\n\\\\nprint(add_to_list(1))\\\\nprint(add_to_list(2))\\\\nprint(add_to_list(3, []))\\\\nprint(add_to_list(4))'\\n```\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:revise_code > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [2.88s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.5-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"name\": \"Code\",\n",
      "                \"arguments\": \"{\\\"code\\\": \\\"def add_to_list(item, my_list=None):\\\\n    if my_list is None:\\\\n        my_list = []\\\\n    my_list.append(item)\\\\n    return my_list\\\\n\\\\nprint(add_to_list(1))\\\\nprint(add_to_list(2))\\\\nprint(add_to_list(3, []))\\\\nprint(add_to_list(4))\\\"}\"\n",
      "              }\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.5-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--13883968-438c-4d60-9b2e-62857aca80e9-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"Code\",\n",
      "                \"args\": {\n",
      "                  \"code\": \"def add_to_list(item, my_list=None):\\n    if my_list is None:\\n        my_list = []\\n    my_list.append(item)\\n    return my_list\\n\\nprint(add_to_list(1))\\nprint(add_to_list(2))\\nprint(add_to_list(3, []))\\nprint(add_to_list(4))\"\n",
      "                },\n",
      "                \"id\": \"dcbb8cd2-68ce-4576-baff-361812ec45b0\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 236,\n",
      "              \"output_tokens\": 578,\n",
      "              \"total_tokens\": 814,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"reasoning\": 469\n",
      "              }\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:revise_code > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:revise_code > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:revise_code > chain:RunnableSequence] [2.88s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:revise_code] [2.89s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Você é um revisor de código estrito. Avalie se o código está correto, legível e executável.\\nSe encontrar problemas, reprove e dê feedback objetivo e curto; se estiver ok, aprove.\\nHuman: Inspecione o código a seguir.\\nResponda com {approved: boolean, feedback: string}.\\n\\n```python\\ncode='def add_to_list(item, my_list=None):\\\\n    if my_list is None:\\\\n        my_list = []\\\\n    my_list.append(item)\\\\n    return my_list\\\\n\\\\nprint(add_to_list(1))\\\\nprint(add_to_list(2))\\\\nprint(add_to_list(3, []))\\\\nprint(add_to_list(4))'\\n```\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [2.96s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"model_name\": \"gemini-2.5-flash\",\n",
      "          \"safety_ratings\": []\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"name\": \"InspectorVerdict\",\n",
      "                \"arguments\": \"{\\\"approved\\\": true}\"\n",
      "              }\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"prompt_feedback\": {\n",
      "                \"block_reason\": 0,\n",
      "                \"safety_ratings\": []\n",
      "              },\n",
      "              \"finish_reason\": \"STOP\",\n",
      "              \"model_name\": \"gemini-2.5-flash\",\n",
      "              \"safety_ratings\": []\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--99a70e1a-1dc6-4018-af22-a7e552eca1c8-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"InspectorVerdict\",\n",
      "                \"args\": {\n",
      "                  \"approved\": true\n",
      "                },\n",
      "                \"id\": \"1ef59f69-5096-4f00-b490-d983434fc1bd\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 241,\n",
      "              \"output_tokens\": 577,\n",
      "              \"total_tokens\": 818,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"reasoning\": 564\n",
      "              }\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    }\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:RunnableSequence] [2.96s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:inspect_routing_function] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector > chain:inspect_routing_function] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"approve\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:inspector] [2.96s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:finalize] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:finalize] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph] [13.05s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"graph_output\": \"Código final aprovado:\\ncode='def add_to_list(item, my_list=None):\\\\n    if my_list is None:\\\\n        my_list = []\\\\n    my_list.append(item)\\\\n    return my_list\\\\n\\\\nprint(add_to_list(1))\\\\nprint(add_to_list(2))\\\\nprint(add_to_list(3, []))\\\\nprint(add_to_list(4))'\"\n",
      "}\n",
      "{'graph_output': \"Código final aprovado:\\ncode='def add_to_list(item, my_list=None):\\\\n    if my_list is None:\\\\n        my_list = []\\\\n    my_list.append(item)\\\\n    return my_list\\\\n\\\\nprint(add_to_list(1))\\\\nprint(add_to_list(2))\\\\nprint(add_to_list(3, []))\\\\nprint(add_to_list(4))'\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.cache.memory import InMemoryCache\n",
    "from langgraph.types import CachePolicy\n",
    "\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True) \n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"AIzaSyBLD9tqggBy_fK_AqjN8g4u3TeLKo344RQ\"\n",
    ")\n",
    "    \n",
    "#saida da llm modelada\n",
    "class InspectorVerdict(BaseModel):\n",
    "    approved: bool = Field(..., description=\"True se o código passar na inspeção; False se reprovar.\")\n",
    "    feedback: str = Field(\"\", description=\"Feedback breve e específico; vazio se aprovado.\")\n",
    "\n",
    "inspector_llm = llm.with_structured_output(InspectorVerdict)\n",
    "\n",
    "class Code(BaseModel):\n",
    "    code: str = Field(\"\", description=\"Apenas o codigo deve estar presente aqui\")\n",
    "\n",
    "code_llm = llm.with_structured_output(Code)\n",
    "\n",
    "#definindo os States\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    user_input: str\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    graph_output: str\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    code: str\n",
    "    inspection_result: bool\n",
    "    feedback: str\n",
    "    user_input: str\n",
    "    graph_output: str\n",
    "\n",
    "#nodes\n",
    "\n",
    "def generate_code_node(state: OverallState):\n",
    "    prompt = state.get(\"user_input\",\"\")\n",
    "    user = HumanMessage(content=(\n",
    "        \"Crie um codigo em python simples com um erro dificil de detectar\\n\"\n",
    "        \"Sua resposta deve conter apenas o codigo\\n\"\n",
    "        \"Responda com {code: string}.\\n\\n\"\n",
    "))\n",
    "    try:\n",
    "        code= code_llm.invoke([user])\n",
    "    except Exception as e:\n",
    "        code = f\"Falha ao inspecionar com LLM: {e}\"\n",
    "    return {\n",
    "        \"code\": code,\n",
    "        \"inspection_result\": False,\n",
    "        \"messages\": [AIMessage(content=f\"Código gerado:\\n{code}\")]\n",
    "    }\n",
    "\n",
    "def inspector_node(state: OverallState) -> dict:\n",
    "    code = state[\"code\"]\n",
    "    system = SystemMessage(content=(\n",
    "        \"Você é um revisor de código estrito. Avalie se o código está correto, legível e executável.\\n\"\n",
    "        \"Se encontrar problemas, reprove e dê feedback objetivo e curto; se estiver ok, aprove.\"\n",
    "    ))\n",
    "    user = HumanMessage(content=(\n",
    "        \"Inspecione o código a seguir.\\n\"\n",
    "        \"Responda com {approved: boolean, feedback: string}.\\n\\n\"\n",
    "        f\"```python\\n{code}\\n```\"\n",
    "    ))\n",
    "    try:\n",
    "        verdict = inspector_llm.invoke([system, user])\n",
    "        approved = bool(verdict.approved)\n",
    "        feedback = verdict.feedback or \"\"\n",
    "        print(feedback)\n",
    "    except Exception as e:\n",
    "        approved = False\n",
    "        feedback = f\"Falha ao inspecionar com LLM: {e}\"\n",
    "\n",
    "    msg = \"Inspeção: aprovado\" if approved else f\"Inspeção: reprovado — {feedback}\"\n",
    "    return {\n",
    "        \"inspection_result\": approved,\n",
    "        \"feedback\": feedback,\n",
    "        \"messages\": [AIMessage(content=f\"[inspector] {msg}\")]\n",
    "    }    \n",
    "\n",
    "def revise_code_node(state: OverallState) -> dict:\n",
    "    code = state[\"code\"]\n",
    "    fb = state.get('feedback',\"\")\n",
    "    system = SystemMessage(content=(\n",
    "        \"Você é um corretor de código. Baseado no feedback a seguir corrija o codigo.\\n\"\n",
    "    ))\n",
    "    user = HumanMessage(content=(\n",
    "        \"Corrija o código a seguir baseado no feedback.\\n\"\n",
    "        \"Responda com {code: string}.\\n\\n\"\n",
    "        f\"```feedback\\n{fb}\\n```\"\n",
    "        f\"```python\\n{code}\\n```\"\n",
    "    ))\n",
    "    try:\n",
    "        fixed =code_llm.invoke([system, user])\n",
    "    except Exception as e:\n",
    "        fixed = f\"Falha ao inspecionar com LLM:{e}\"    \n",
    "    return {\n",
    "    \"code\": fixed,\n",
    "    \"inspection_result\": False, \n",
    "    \"messages\": [AIMessage(content=f\"[reviser] Revisado conforme feedback ('{fb}'):\\n{fixed}\")]\n",
    "}\n",
    "    \n",
    "\n",
    "def finalize_node(state: OverallState) -> dict:\n",
    "    return {\n",
    "        \"graph_output\": f\"Código final aprovado:\\n{state['code']}\",\n",
    "        \"messages\": [AIMessage(content=\"[finalize] Fluxo finalizado.\")]\n",
    "    }\n",
    "\n",
    "#edge condicional\n",
    "def inspect_routing_function(state):\n",
    "    return \"approve\" if state.get(\"inspection_result\") else \"revise\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "builder = StateGraph(\n",
    "    OverallState,\n",
    "    input_schema=InputState,\n",
    "    output_schema=OutputState\n",
    ")\n",
    "\n",
    "\n",
    "builder.add_node(\"generate_code\", generate_code_node)\n",
    "builder.add_node(\"inspector\", inspector_node)\n",
    "builder.add_node(\"revise_code\", revise_code_node)\n",
    "builder.add_node(\"finalize\", finalize_node)\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"generate_code\")\n",
    "builder.add_edge(\"generate_code\", \"inspector\")\n",
    "builder.add_conditional_edges(\n",
    "    \"inspector\",\n",
    "    inspect_routing_function,\n",
    "    {\"approve\": \"finalize\", \"revise\": \"revise_code\"}\n",
    ")\n",
    "builder.add_edge(\"revise_code\", \"inspector\")\n",
    "builder.add_edge(\"finalize\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "print(graph.invoke({\"user_input\": \"NAO ESTAMOS USANDO PROMPT AGR\"}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
